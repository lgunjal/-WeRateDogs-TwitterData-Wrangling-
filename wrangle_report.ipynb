{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "> The analysis is regarding tweet dataset of a we rate dogs archive. The source of this data is from twitter API which was programatically downloaded using Twitter API into a txt file named tweet_json.txt , a tsv file which contained Neural network image predictions data for dog images that were uploaded to this WeRateDogs Archive, the predicitons included 9 columns describing 3 different probabilities of the image being of a certain breed of a dog or another object and a True/False column indicating if the prediction was a dog or not and then a column that indicated the dog breed. Another source was the tweet enhamced archive csv file provided to us. This contained some tweet data, favorite counts,likes, retweets, image urls and tweets urls. each of these sources were imported into python interface using pandas package and a dataframe was created for each of these data sources. It was then joined with each other using the tweet id as a key to form a master dataframe called the master. A copy of the master dataframe called as master1 was created to go through all cleaning processes.\n",
    "\n",
    ">There were many issues with cleanliness and tidiness. quality issues included some records without dog breeds, some records having dogs rating denominator other than 10 on the scale which were replaced by 10, wrong dog names which were replaced by blanks, timestamp values ending with 0000 which was removed, data format of timestamp column was changed from string to datetime, tweets without images were removed, text columns with multiple urls were segregated and put into two new columns, expanded urls were also divided into 3 columns. Besides, the source columns which had url links were converted into readable source categories, and the two columns in_reply_to_status_id and in_reply_to_user_id were removed beacuse they had many blanks and were not of any use for our analysis. \n",
    "\n",
    ">Finally a CSV file was created consisting of clean data and named 'twitter_archive_master.csv'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
